# Краткое описание
[Ссылка на официальный сайт хакатона](https://changellenge.com/championships/aiforfinancehack/)

В рамках хакатона AI for Finance Hack разработала AI-ассистента на базе LLM.
Он отвечает на вопросы клиентов банка, используя базу знаний из 350 статей — от вкладов и кредитов до инвестиций и страхования.

# Контекст и цель
Банковские клиенты ежедневно обращаются с сотнями однотипных вопросов, а колл-центры и шаблонные чат-боты требуют больших ресурсов и не всегда дают корректные ответы.

Наша цель — создать ассистента, который понимает запрос пользователя и отвечает точно, опираясь на достоверную базу знаний.

# Инструменты и технологии
Python, LLM, RAG, API

# Этапы работы
1. Подготовка
    - Изучили принципы работы RAG и варианты построения ассистентов
    - Определили архитектуру и этапы проекта
2. Подготовка данных
    - Загрузили базу из 350 статей (CSV)
    - Очистили текст от разметки, переносов строк и лишних символов
3. Разбиение на чанки
    - Определили оптимальный размер чанка
    - Разбили статьи и собрали DataFrame с контекстом, тегами и аннотациями
4. Эмбенддинги
    - Сгенерировали эмбеддинги для всех чанков
    ![эмбенддинги](https://github.com/Olysechka02/Portfolio/blob/main/Материалы/эмб.png)
    - Сохранили их в общий датафрейм для последующего поиска
5. Обработка вопросов
    - Подготовили и очистили список пользовательских вопросов
6. Обработка запросов к LLM
    - подключились к 2ум разным моделям: первая - эмбендит вопрос, другая - генерит ответ на промт
    - Для каждого вопроса:
        1. Эмбенддинг вопроса;
        2. Поиск релевантных чанков;
        3. Формирование контекста;
        ![часть скрипта](https://github.com/Olysechka02/Portfolio/blob/main/Материалы/ближдок.png)<br>
        4. Создание промта;
        5. Отправка в модель;
        6. Сохранение результата;
7. Финальный этап
    - Сохранили результаты в два CSV-файла:
        1. Полный (id, вопрос, контекст, ответ)
        2. Итоговый (id, вопрос, ответ)

### Оптимизация
- Эмбеддинги создавались пакетно (batching)
- Добавили кеширование на этапах:

    1. создания чанков
    2. генерации эмбеддингов
    3. формирования датафреймов.
- Реализовали промежуточный вывод логов в консоль.

### Тестирование:
- разные длины чанков
- разная детализация промта
- различные алгоритмы поиска ближайших ответов

# Результат
    - [Готовый скрипт](https://github.com/Olysechka02/Portfolio/blob/main/Материалы/main.py), генерирующий точные ответы с использованием LLM и RAG
    - CSV-файл, содержащий сформированные ответы на поставленные вопросы

# Чему я научилась:
    - Разобралась в структуре RAG и реализовала её на практике
    - Освоила LangChain и работу с API LLM
    - Поняла, как улучшать точность ответов с помощью корректного контекста
