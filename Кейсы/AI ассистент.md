# Краткое описание
[Ссылка на официальный сайт хакатона](https://changellenge.com/championships/aiforfinancehack/)

В рамках хакатона стояла задача создать финансового асситента. Разработали ассистента на основе LLM, который  выдает ответы на вопросы пользователей, опираясь на базу знаний из 350 статей

# Контекст и цель
Клиенты банков ежедневно задают множество вопросов о продуктах и услугах: по вкладам, кредитам, комиссиям, инвестициям, страхованию. Обработка этих запросов через колл-центры и чат-боты требует значительных вычислительных и человеческих ресурсов, а качество зависит от уровня подготовки сотрудников или ограниченности шаблонов ответов. Использование AI-ассистентов может помочь, но важно гарантировать правильность их ответов.

Задача:
Разработать ассистента на основе LLM, который использует базу знаний для ответов на вопросы пользователей

# Инструменты и технологии
Python, LLM, RAG, API

# Этапы работы
1. Подготовительный этап
    - Узнали, как работает RAG
    - Какие способы построения ассистенов бывают
    - Наметили план работ
2. Очистка данных
    - загрузили из CSV список статей (базу знаний)
    - убрали лишние символы (переносы строк, markdown разметку, двойные пробелы и тп)
3. Разбиение на чанки
    - определили размер одного чанка
    - разбили все статьи на чанки
    - создали датафрейм, где лежат все чанки и вся изначально прилагающаяся информация к ним (теги, аннотация статьи, сама статья)
4. Эмбендинг чанков
    - подключились к модели, которая генерирует эмбендинги
    - отправили в неё все чанки
    - сохранили все эмбендинги в общий датафрейм
5. Обработка вопросов
    - загрузили из CSV список вопросов
    - убрали лишние символы (переносы строк, markdown разметку, двойные пробелы и тп)
6. Отправка запроса к LLM
    - подключились к 2ум разным моделям: первая эмбендит вопрос, другая генерит ответ на промт
    - в цикле:
        1. отправляю вопрос на эмбендинг, 
        2. подбираю наиболее подходящие куски к вопросу, 
        3. записываю эти куски в контекст
        4. пишу промт + соединяю контекст с промтом
        5. отправляю в LLM эту структуру
        6. Сохраняю ответ в список
7. Обработка выходного файла
    - сохраняем в CSV файл вопрос, контекст, ответ (для анализа контекста и ответов)
    - и сохраняем в другой csv файл итоговый ответ: id вопроса, вопрос, ответ на вопросы


### Для оптимизации использовали: 
- создание эмбендингов бакетами
- использовали кеш на этапах 
    1. Создание чанков
    2. Создание эмбендингов
    3. Создание итогового датафрейма
- выводим в консоль результат всех промежуточных этапов

### Что тестировали:
    - разную длину чанков статей
    - разной длины и детализации промт
    - разный алгоритм находжения ближайших ответов на вопрос

# Результат
    - рабочий скрипт, который генерирует правильный и точный ответ на вопрос пользователя, используя API LLM
    - CSV-файл, содержащий сформированные ответы на поставленные вопросы

# Чему я научилась:
    - узнала структуру построения RAG, воссоздала её
    - научилась работать с библиотеками langchain
    - научилась работать с API OpenAI
